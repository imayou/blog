---
layout: docker、kubernate
title: 06-白话容器基础（二）：隔离与限制
date: 2019-04-30 20:18:33
tags:
---
配图：
在上一篇的图中不应该吧Docker Engine或者任何容器管理工具放在跟Hypervisor相同的位置，因为他们并不像Hypervisor那样对应用进程的隔离环境负责，也不会创建任何实体“容器”，真正对隔离环境负责的是宿主机操作系统本身：（图）
> 我们应该把Docker画在跟应用同级别并且靠边的位置。意味着用户运行在容器里的应用进程，和宿主机上的其他进程一样，都由操作系统统一管理，只不过这些被隔离的进程多了额外设置的Namespace参数，而Docker扮演的只是旁路式的辅助和管理工作。

docker比虚拟机受欢迎的原因：资源
- 虚拟机作为应用沙盒，必须由Hypervisor负责创建虚拟机，真实存在的虚拟机里面必须运行完整的Guest OS才能运行应用进程，带来额外的资源消耗和占用。
- 容器只是宿主机上的一个普通进程，不用虚拟化带来新能损耗，而且Namespace作为隔离手段不需单独的Guest OS，使得容器额外的资源占用可以忽略不计。

缺点-隔离不彻底：
- 容器只是宿主机的一个特殊进程，多个容器共享一个宿主机操作系统内核
- 很多资源和对象不能被Namespace化，例如时间 意味着在容器中的程序使用`settimeofday(2)`系统调用修改了时间，整个宿主机的时间都会被修改
- 可用Seccomp等技术进行系统调用的过滤和甄别达到安全加固，但是拖累了容器性能，默认情况下不知道该开启禁用哪些系统调用


> 容器里的一个应用进程既然对应宿主的一个进程，所以会和宿主其他进程平等竞争资源（比如CPU、内存），显然不是一个“沙盒”应该表现出来的合理行为，而Linux Cgroup(全称：Linux Control Group)就是linux内核中用来为进程设置资源限制的一个重要功能，可以限制包括CPU、内存、磁盘、网络带宽等等。

在Linux中，Cgroup给用户暴露出来的操作接口是文件系统，以文件和目录的方式组织在系统的/sys/fs/cgroup路径下，centos root用户下：
```
[root@MyCloudServer ~]# mount -t cgroup
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu)
cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)
cgroup on /sys/fs/cgroup/net_cls type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls)
cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)
cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)
cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)
cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)
cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)
cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)
```
输出结果是一系列文件系统目录，如果没有看到自己的目录，那就需要自己去挂载Cgroups

可以看到在/sys/fs/cgroup下有很多诸如cpuset、cpu、memory这样的子目录（也叫子系统）。你看到的均为机器当前可被Cgroups进行限制的资源种类，对应的种类下可以看到具体可被限制的方法，比如cpu子系统：
```
[root@MyCloudServer ~]# ls /sys/fs/cgroup/cpu
cgroup.clone_children  cgroup.procs          cpuacct.stat   cpuacct.usage_percpu  cpu.cfs_quota_us  cpu.rt_runtime_us  cpu.stat           release_agent  tasks
cgroup.event_control   cgroup.sane_behavior  cpuacct.usage  cpu.cfs_period_us     cpu.rt_period_us  cpu.shares         notify_on_release  system.slice   user.slice
```
上面的cfs_period和cfs_quota组合使用可以用来限制进程在长度为cfs_period的一段时间内，只能被分配到总量为cfs_quota的CPU时间。

使用：
- 需要在子系统下创建目录，比如进入/sys/fs/cgroup/cpu目录下：
```
[root@MyCloudServer cpu]# mkdir container
[root@MyCloudServer cpu]# ls container/
cgroup.clone_children  cgroup.procs  cpuacct.usage         cpu.cfs_period_us  cpu.rt_period_us   cpu.shares  notify_on_release
cgroup.event_control   cpuacct.stat  cpuacct.usage_percpu  cpu.cfs_quota_us   cpu.rt_runtime_us  cpu.stat    tasks
```
> 这个目录就称为一个“控制组”，系统会在你创建的container目录下自动生成对应的资源限制文件。

- 向container组里的cfs_quota文件写入20ms（20000us）：
```
[root@MyCloudServer ~]# echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 
[root@MyCloudServer ~]# cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 
20000
```
> 意味着100ms的时间里，被该控制组限制的进程只能使用20ms的cpu时间，也就是说这个进程只能使用20%的cpu带宽

- 把被限制的进程PID写入container组里的tasks文件，上面的设置就生效了
```
[root@MyCloudServer ~]# echo 5445 > /sys/fs/cgroup/cpu/container/tasks 
```

除了CPU子系统外，Cgroups的每一项子系统都有其独有的资源限制能力，比如：
- blkio，为块设备设定I/O限制，一般用于磁盘等设备；
- cpuset，为进程分配单独的CPU核和对应的内存节点；
- memory，为进程设定内存使用的限制

简单粗暴3步骤，创建子目录，设定参数，加PID到tasks里
至于这些控制组下面的资源文件里填上什么值，就靠用户执行docker run时的参数指定了：
```
docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
```
启动这个容器后，我们可以通过查看Cgroups文件系统，CPU子系统中，“docker”这个控制组里的资源限制文件的内容来确认：
```
[root@MyCloudServer ~]# cat /sys/fs/cgroup/docker/5d5c9f67d/cpu.cfs_quota_us
20000
[root@MyCloudServer ~]# cat /sys/fs/cgroup/docker/5d5c9f67d/cpu.cfs_period_us
100000
```
